// const cron = require('node-cron');
// const s3 = require('./spacesClient');
// const fs = require('fs');
// const { processAllRawMaps } = require('./processSpaceMap'); // ‚úÖ Import updated function

// cron.schedule('* * * * * *', async () => {
//   console.log('‚è≥ Cron: scanning lidar/zips/ for new uploads...');
  
//   // ‚úÖ Process all tar files automatically
//   await processAllRawMaps();

//   // ‚úÖ Backup logic stays as-is
//   try {
//     const fileName = 'backup.json';
//     const fileContent = fs.readFileSync(`./backups/${fileName}`);

//     await s3.putObject({
//       Bucket: process.env.DO_SPACES_BUCKET,
//       Key: fileName,
//       Body: fileContent,
//       ACL: 'private'
//     }).promise();

//     console.log('Backup uploaded successfully!');
//   } catch (err) {
//     console.error('Error uploading backup:', err);
//   }
// });


const cron = require('node-cron');
const s3 = require('./spacesClient');
const fs = require('fs');
const { processAllRawMaps } = require('./processSpaceMap'); // ‚úÖ Import updated function

// ‚úÖ Run immediately when the server starts
(async () => {
  console.log('üöÄ Server started: processing existing raw maps...');
  await processAllRawMaps();
  console.log('‚úÖ Initial processing completed.');
})();

// ‚úÖ Schedule cron job to run every minute (not every second)
cron.schedule('* * * * *', async () => {
  console.log('‚è≥ Cron: scanning lidar/zips/ for new uploads...');
  
  // ‚úÖ Process all tar files automatically
  await processAllRawMaps();

  // ‚úÖ Backup logic stays as-is
  try {
    const fileName = 'backup.json';
    const fileContent = fs.readFileSync(`./backups/${fileName}`);

    await s3.putObject({
      Bucket: process.env.DO_SPACES_BUCKET,
      Key: fileName,
      Body: fileContent,
      ACL: 'private'
    }).promise();

    console.log('Backup uploaded successfully!');
  } catch (err) {
    console.error('Error uploading backup:', err);
  }
});
